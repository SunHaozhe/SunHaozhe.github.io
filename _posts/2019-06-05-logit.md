---
title: Logit
description: >
  Statistics
---

One problem of terminology: logit

In statistics, the logit (/ˈloʊdʒɪt/ LOH-jit) function or the log-odds is the logarithm of the odds p/(1 − p) 
where p is the probability.[1]. It is a type of function that creates a map of probability values from [0,1] to 
[-\infty, +\infty]. It is the inverse of the standard logistic function (sigmoid).

In deep learning, the term logits layer is popularly used for the last neuron layer of neural networks used for 
classification tasks, which produce raw prediction values as real numbers ranging from [-\infty, +\infty].

When one talks about deep learning for classification tasks, the output values of the next to last layer 
(the layer before the final softmax activation) are called logits.


History of this term:

There have been several efforts to adapt linear regression methods to domain where output is probability value [0,1] instead of any real number [-\infty, +\infty]. Many of such efforts focused on modeling this problem by somehow mapping the range [0,1] to [-\infty, +\infty] and then running the linear regression on these transformed values. In 1934 Chester Ittner Bliss used the cumulative normal distribution function to perform this mapping and called his model probit an abbreviation for "probability unit". However, this is computationally more expensive. ***In 1944, Joseph Berkson used log of odds and called this function logit, abbreviation for "logistic unit" following the analogy for probit***. Log odds was used extensively by Charles Sanders Peirce (late 19th century). G. A. Barnard in 1949 coined the commonly used term log-odds; the log-odds of an event is the logit of the probability of the event.

So "logit" actually means "logistic unit".
